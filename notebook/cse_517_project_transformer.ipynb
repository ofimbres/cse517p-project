{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vbt1i9kCr_93",
        "outputId": "8638132c-b87d-4b9d-fd24-8d117b935f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import string\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import unicodedata\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import unicodedata\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import unicodedata\n",
        "import random\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset"
      ],
      "metadata": {
        "id": "FtYQPPx0wYEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, num_heads=4, hidden_dim=256, num_layers=2, max_len=100):\n",
        "        super(CharTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_len, embedding_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=hidden_dim,dropout=0.1, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        x = self.embedding(x) + self.pos_embedding[:, :seq_len, :]\n",
        "        out = self.transformer(x)\n",
        "        logits = self.fc(out[:, -1, :])  # Use the last token's output for prediction\n",
        "        return logits, None"
      ],
      "metadata": {
        "id": "LZGiUG2-SksX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel:\n",
        "    \"\"\"\n",
        "    This is a starter model to get you started. Feel free to modify this file.\n",
        "    \"\"\"\n",
        "    vocab = sorted(set(string.printable))\n",
        "    char2idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
        "    idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
        "    vocab_size = len(vocab)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def __init__(self,char2idx, idx2char, vocab_size):\n",
        "        #self.model = CharLSTM(self.vocab_size).to(self.device)\n",
        "        self.char2idx = char2idx\n",
        "        self.idx2char = idx2char\n",
        "        self.vocab_size = vocab_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = CharTransformer(self.vocab_size).to(self.device)\n",
        "\n",
        "    def load_oscar_char_data(\n",
        "      lang_code='en',\n",
        "      seq_len=30,\n",
        "      sample_size=10000,\n",
        "      pair_fraction=0.2,\n",
        "      test_split=0.1,\n",
        "      top_k_chars=2000\n",
        "    ):\n",
        "      \"\"\"\n",
        "      Stream OSCAR dataset (for a specific language), normalize it, and return character-level training pairs.\n",
        "      \"\"\"\n",
        "      print(f\"ðŸ”„ Streaming OSCAR ({lang_code}) from Hugging Face Hub...\")\n",
        "      dataset = load_dataset(\n",
        "          \"oscar\",\n",
        "          f\"unshuffled_deduplicated_{lang_code}\",\n",
        "          split=\"train\",\n",
        "          streaming=True\n",
        "      )\n",
        "\n",
        "      def normalize(text):\n",
        "          text = unicodedata.normalize(\"NFKC\", text)\n",
        "          return ''.join(c for c in text if not unicodedata.category(c).startswith('C'))\n",
        "\n",
        "      print(\"ðŸ“ Collecting and normalizing text...\")\n",
        "      buffer = \"\"\n",
        "      count = 0\n",
        "      for sample in tqdm(dataset, total=sample_size*2):\n",
        "          raw = sample.get(\"text\", \"\").strip()\n",
        "          if raw:\n",
        "              buffer += normalize(raw) + \"\\n\"\n",
        "              count += 1\n",
        "          if count >= sample_size:\n",
        "              break\n",
        "\n",
        "      print(f\"âœ… Collected text: {len(buffer):,} characters from {count} samples\")\n",
        "\n",
        "      # Build character vocabulary\n",
        "      counter = Counter(buffer)\n",
        "      vocab = [c for c, _ in counter.most_common(top_k_chars)]\n",
        "      char2idx = {ch: i for i, ch in enumerate(vocab)}\n",
        "      idx2char = {i: ch for ch, i in char2idx.items()}\n",
        "\n",
        "      # Generate (context, next_char) pairs\n",
        "      num_possible = len(buffer) - seq_len\n",
        "      num_pairs = int(num_possible * pair_fraction)\n",
        "      if num_pairs < 1:\n",
        "          num_pairs = 1\n",
        "\n",
        "      indices = random.sample(range(num_possible), num_pairs)\n",
        "      data = []\n",
        "      for i in tqdm(indices, desc=\"ðŸ”— Generating training pairs\"):\n",
        "          context = buffer[i:i+seq_len]\n",
        "          next_char = buffer[i+seq_len]\n",
        "          if all(c in char2idx for c in context + next_char):\n",
        "              data.append((context, next_char))\n",
        "\n",
        "      print(f\"ðŸŸ¢ Total training pairs: {len(data)}\")\n",
        "      train_data, _ = train_test_split(data, test_size=test_split, random_state=42)\n",
        "\n",
        "      return train_data, vocab, char2idx, idx2char\n",
        "\n",
        "    @classmethod\n",
        "    def load_test_data(cls, fname, sequence_length=10):\n",
        "        # your code here\n",
        "        data = []\n",
        "        with open(fname) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                #inp = line[:-1]  # the last character is a newline\n",
        "                #data.append(inp)\n",
        "                for i in range(len(line) - sequence_length):\n",
        "                    context = line[i:i+sequence_length]\n",
        "                    next_char = line[i+sequence_length]\n",
        "                    if all(c in cls.char2idx for c in context + next_char):\n",
        "                        data.append((context, next_char))\n",
        "        return data\n",
        "\n",
        "    @classmethod\n",
        "    def write_pred(cls, preds, fname):\n",
        "        with open(fname, 'wt') as f:\n",
        "            for p in preds:\n",
        "                f.write('{}\\n'.format(p))\n",
        "\n",
        "    def run_train(self, data, work_dir, epochs=3, lr=0.003, batch_size=64):\n",
        "      # Prepare tensors\n",
        "      X = torch.tensor([[self.char2idx[c] for c in context] for context, _ in data], dtype=torch.long)\n",
        "      y = torch.tensor([self.char2idx[target] for _, target in data], dtype=torch.long)\n",
        "\n",
        "      dataset = TensorDataset(X, y)\n",
        "      dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "      # Loss and optimizer\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "      print('Training on {} samples'.format(len(data)))\n",
        "      print('Using device {}'.format(self.device))\n",
        "      self.model.train()\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "          print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "          total_loss = 0.0\n",
        "\n",
        "          for x_batch, y_batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "              x_batch = x_batch.to(self.device)\n",
        "              y_batch = y_batch.to(self.device)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              logits, _ = self.model(x_batch)\n",
        "              loss = criterion(logits, y_batch)\n",
        "              loss.backward()\n",
        "\n",
        "              # ðŸš¨ Gradient clipping (important for LSTM)\n",
        "              torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "              optimizer.step()\n",
        "              total_loss += loss.item() * x_batch.size(0)  # Multiply by batch size for total\n",
        "\n",
        "          avg_loss = total_loss / len(dataset)\n",
        "          print(f\"Epoch {epoch+1}/{epochs} - Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "      # Save model\n",
        "      torch.save({\n",
        "          'model_state_dict': self.model.state_dict(),\n",
        "          'char2idx': self.char2idx,\n",
        "          'idx2char': self.idx2char,\n",
        "          'vocab_size': self.vocab_size\n",
        "      }, os.path.join(work_dir, 'model.pt'))\n",
        "\n",
        "\n",
        "    def run_pred(self, data):\n",
        "        # your code here\n",
        "        self.model.eval()\n",
        "        preds = []\n",
        "        all_chars = string.ascii_letters\n",
        "        with torch.no_grad():\n",
        "            for context, _ in data:\n",
        "                x = torch.tensor([[self.char2idx.get(c, 0) for c in context]], dtype=torch.long).to(self.device)\n",
        "                logits, _ = self.model(x)\n",
        "                probs = torch.softmax(logits, dim=-1)\n",
        "                top3 = torch.topk(probs, 3).indices[0].tolist()\n",
        "                pred_chars = [self.idx2char[i] for i in top3]\n",
        "                preds.append(''.join(pred_chars))\n",
        "        return preds\n",
        "        #for inp in data:\n",
        "         #   # this model just predicts a random character each time\n",
        "          #  top_guesses = [random.choice(all_chars) for _ in range(3)]\n",
        "           # preds.append(''.join(top_guesses))\n",
        "        #return preds\n",
        "\n",
        "    def save(self, work_dir):\n",
        "        # your code here\n",
        "        # this particular model has nothing to save, but for demonstration purposes we will save a blank file\n",
        "        with open(os.path.join(work_dir, 'model.checkpoint'), 'wt') as f:\n",
        "            f.write('dummy save')\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, work_dir):\n",
        "        # your code here\n",
        "        # this particular model has nothing to load, but for demonstration purposes we will load a blank file\n",
        "        #with open(os.path.join(work_dir, 'model.checkpoint')) as f:\n",
        "         #   dummy_save = f.read()\n",
        "        #return MyModel()\n",
        "        model = cls()\n",
        "        checkpoint = torch.load(os.path.join(work_dir, 'model.pt'), map_location=cls.device)\n",
        "        model.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.model.to(cls.device)\n",
        "        model.model.eval()\n",
        "        return model"
      ],
      "metadata": {
        "id": "SeaSWwLZUoS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Instatiating model')\n",
        "\n",
        "print('Loading training data')\n",
        "#train_data = MyModel.load_training_data()\n",
        "#train_data, vocab, char2idx, idx2char = MyTransformerModel.load_oscar_char_data()\n",
        "train_data, vocab, char2idx, idx2char = MyModel.load_oscar_char_data(\n",
        "\n",
        ")\n",
        "model = MyModel(char2idx, idx2char, len(vocab))\n",
        "print('Training')\n",
        "model.run_train(train_data, './work')\n",
        "print('Saving model')\n",
        "model.save('./work')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q89zeHGEPDlg",
        "outputId": "c6a879f3-3d4d-4955-e253-d8e6e59bee8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instatiating model\n",
            "Loading training data\n",
            "ðŸ”„ Streaming OSCAR (en) from Hugging Face Hub...\n",
            "ðŸ“ Collecting and normalizing text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 9999/20000 [00:16<00:16, 601.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Collected text: 48,337,613 characters from 10000 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ðŸ”— Generating training pairs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9667516/9667516 [00:27<00:00, 348380.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŸ¢ Total training pairs: 9665205\n",
            "Training\n",
            "Training on 8698684 samples\n",
            "Using device cuda\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135917/135917 [16:10<00:00, 140.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 - Avg Loss: 2.5234\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135917/135917 [16:11<00:00, 139.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3 - Avg Loss: 2.4392\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135917/135917 [16:12<00:00, 139.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3 - Avg Loss: 2.3538\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}